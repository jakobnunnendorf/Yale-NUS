{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 10: Logistic Regression\n",
    "\n",
    "In this lab you will build a logistic regression model and evaluate the performance of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "#import cufflinks as cf\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "#py.init_notebook_mode(connected=False)\n",
    "#cf.set_config_file(offline=False, world_readable=True, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "dataset",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In this lab we will be working with the breast cancer dataset. This dataset can be loaded using the `sklearn.datasets.load_breast_cancer()` method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "loaddata",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "data = sklearn.datasets.load_breast_cancer()\n",
    "# data is actually a dictionnary\n",
    "print(data.keys())\n",
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format is not a `pandas.DataFrame` so we will need to do some preprocessing to create a new DataFrame from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "dataframe",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "fitone",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let us try to fit a simple model with only one feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "buildxy",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Define our features/target\n",
    "X = df[[\"mean radius\"]]\n",
    "# Target data['target'] = 0 is malignant, 1 is benign\n",
    "Y = (data.target == 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "split",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Create a 75-25 train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,y_train,y_test = train_test_split(X,Y, test_size=0.25, random_state=42)\n",
    "\n",
    "print(f\"Training Data Size: {len(x_train)}\")\n",
    "print(f\"Test Data Size: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "q1_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1\n",
    "\n",
    "Let's first fit a logistic regression model using the training set. \n",
    "\n",
    "For this problem, we will use the existing `LogisticRegression` implementation in sklearn.\n",
    "\n",
    "Fill in the code below to compute the training and testing accuracy, defined as:\n",
    "\n",
    "$$\n",
    "\\text{Training Accuracy} = \\frac{1}{n_{train\\_set}} \\sum_{i \\in {train\\_set}} {\\mathbb{1}_{y_i == \\hat{y_i}}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Testing Accuracy} = \\frac{1}{n_{test\\_set}} \\sum_{i \\in {test\\_set}} {\\mathbb{1}_{y_i == \\hat{y_i}}}\n",
    "$$\n",
    "\n",
    "where $\\hat{y_i}$ is the prediction of our model, $y_i$ the true value, and $\\mathbb{1}_{y_i == \\hat{y_i}}$ an indicator function where $\\mathbb{1}_{y_i == \\hat{y_i}} = 1$ if ${y_i} = \\hat{y_i}$, and $\\mathbb{1}_{y_i == \\hat{y_i}} = 0$ if ${y_i} \\neq \\hat{y_i}$\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = sklearn.linear_model.LogisticRegression(fit_intercept=True, solver = 'lbfgs')\n",
    "\n",
    "lr.fit(x_train,y_train) \n",
    "train_accuracy = ...\n",
    "test_accuracy = ...\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_text",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2\n",
    "It seems we can get a very high test accuracy. How about precision and recall?  \n",
    "- Precision (also called positive predictive value) is the fraction of true positives among the total number of data points predicted as positive.  \n",
    "- Recall (also known as sensitivity) is the fraction of true positives among the total number of data points with positive labels.\n",
    "\n",
    "Precision measures the ability of our classifier to not predict negative samples as positive, while recall is the ability of the classifier to find all the positive samples.\n",
    "\n",
    "To understand the link between precision and recall, it's useful to create a confusion matrix of our predictions. Luckily, `sklearn.metrics` provides us with such a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_display",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, lr.predict(x_test))\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "class_names = ['False', 'True']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plt.grid(False)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plt.grid(False)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_text2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Mathematically, Precision and Recall are defined as:\n",
    "$$\n",
    "\\text{Precision} = \\frac{n_{true\\_positives}}{n_{true\\_positives} + n_{false\\_positives}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{Recall} = \\frac{n_{true\\_positives}}{n_{true\\_positives} + n_{false\\_negatives}}\n",
    "$$\n",
    "\n",
    "Below is a graphical illustration of precision and recall:\n",
    "![precision_recall](precision_recall.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-sd",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now let's compute the precision and recall for the test set using the model we got from Question 1.  \n",
    "\n",
    "**Do not** use `sklearn.metrics` for this computation.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(x_test) \n",
    "\n",
    "...\n",
    "\n",
    "precision = ...\n",
    "recall = ...\n",
    "\n",
    "print(f'precision = {precision:.4f}')\n",
    "print(f'recall = {recall:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_question",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Our precision is fairly high while our recall is a bit lower. Why might we observe these results? Please consider the following plots, which display the distribution of the target variable in the training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_display2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2)\n",
    "sns.countplot(x=y_train, ax=axes[0]);\n",
    "sns.countplot(x=y_test, ax=axes[1]);\n",
    "\n",
    "axes[0].set_title('Train')\n",
    "axes[1].set_title('Test')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit your assignment, please download your notebook as a .ipynb file and submit to Canvas. You can do so by navigating to the toolbar at the top of this page, clicking File > Download as... > Notebook (.ipynb) or HTML (.html). Then, upload both files under \"Lab #10\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
